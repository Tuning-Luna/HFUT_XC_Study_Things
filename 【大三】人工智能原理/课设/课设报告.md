# 本地大模型部署与RAG课设报告


## 1. 项目概述
- 目标：完成本地化大模型部署、基础问答、RAG 检索增强问答。
- 技术栈：Ollama + Qwen2、Python、LangChain、FAISS、Sentence-Transformers。
- 代码位置：`setup_model.py`（部署），`simple_qa.py`（基础问答），`rag_qa.py`（RAG）。

## 2. 环境与依赖
- 硬件：推荐 16GB 内存；有 NVIDIA GPU 可加速。
- 软件：Python 3.9+；Ollama（本地模型运行时）。
- 依赖安装：`pip install -r requirements.txt`
- 模型：`qwen2:7b`（可改 `qwen2:1.5b` 节省资源）。

## 3. 任务1：本地化大模型部署
1) 安装 Ollama  
   - Windows 下载安装包：https://ollama.com/download  
2) 拉取模型：`python setup_model.py`  
3) 脚本流程（`setup_model.py`）：  
   - 检查 `ollama --version`；未安装则提示。  
   - `ollama pull qwen2:7b` 拉取模型。  
   - 用 `ollama.chat` 发送自检问题验证推理。
4) 结果：终端输出模型版本与首轮回答，表示部署成功。

## 4. 任务2：调用 API 实现问答
文件：`simple_qa.py`  
- 功能：单轮问答与多轮对话，维护历史消息列表。  
- 关键调用：`ollama.chat(model="qwen2:7b", messages=[...])`。  
- 运行：`python simple_qa.py`，输入问题，`quit/exit/退出` 结束。  
- 预期：返回中文回答，多轮模式会基于历史上下文作答。

## 5. 任务3：RAG 检索增强问答（LangChain + FAISS）
文件：`rag_qa.py`  
- 流程：
  1) 文档加载：支持 `.pdf`/`.txt`（`documents` 目录）。示例已提供 `documents/示例文档.txt`。  
  2) 分块：`RecursiveCharacterTextSplitter`，默认 `chunk_size=500`，`chunk_overlap=50`。  
  3) 向量化：`HuggingFaceEmbeddings` 使用 `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`（CPU，可改 GPU）。  
  4) 向量库：`FAISS.from_documents` 建库，持久化到 `faiss_index/`。  
  5) 检索与生成：`RetrievalQA`，`k=3`，提示词要求“不知道就直说”。  
- 首次运行：`python rag_qa.py`，自动建库并保存；后续运行会直接加载现有索引。  
- 预期输出：答案 + 相关文档块计数，便于核对出处。

## 6. 测试与结果
- 部署验证：`setup_model.py` 输出模型自检回答。  
- 基础问答示例：问 “什么是人工智能？”，模型返回简述定义与要点。  
- RAG 示例：问 “人工智能的主要研究领域有哪些？”，回答应包含示例文档中的 5 个方向；检索返回 3 个相关块。

## 7. 优化与扩展
- 资源优化：将模型改为 `qwen2:1.5b` 或减少 `chunk_size`。  
- 检索优化：调大/调小 `k`，或用更强嵌入模型。  
- 安全性：提示词限制“未知则直说”，可加入长度截断与敏感词过滤。  
- 部署扩展：如需 HTTP 服务，可用 FastAPI 包装 `simple_qa.py`/`rag_qa.py`。

## 8. 遇到的问题与解决
- 模型下载慢：使用代理或换小模型。  
- 内存占用高：降低模型尺寸，或分批加载文档。  
- 编码/文件格式：仅支持 UTF-8 的 `.pdf`/`.txt`，其余需先转换。  
- 依赖导入警告：确保运行前已执行 `pip install -r requirements.txt`。

## 9. 总结
- 已完成本地部署、API 问答与 RAG 三项要求。  
- 方案简单可复现，核心脚本 3 个，依赖少、配置轻。  
- 可直接将本报告连同代码目录提交，必要时补充实验截图与性能数据。

